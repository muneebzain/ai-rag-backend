# AI RAG Backend

Production-ready Retrieval-Augmented Generation (RAG) backend built with FastAPI and a local Ollama LLM.

This system combines semantic retrieval, hybrid ranking, grounding validation, and streaming responses to deliver reliable AI answers.

---

## Features

* Document ingestion with chunking + overlap
* Embedding generation
* Vector similarity search
* Hybrid semantic + keyword ranking
* Context length limiting
* Confidence scoring
* Semantic grounding validation
* Hallucination refusal logic
* In-memory caching
* Streaming token responses
* Performance metrics tracking
* Local LLM inference using Ollama

---

## Tech Stack

* Python 3.12+
* FastAPI
* Ollama (Local LLM)
* Vector Search
* Custom Grounding Layer
* SwiftUI client (separate repo)

---

## Architecture

iOS App
→ FastAPI Backend (Port 8000)
→ Ollama Local Model (Port 11434)

### Pipeline

User Question
→ Query Embedding
→ Vector Search
→ Hybrid Ranking
→ Context Assembly
→ Prompt Construction
→ Local LLM (Ollama)
→ Grounding + Confidence Evaluation
→ Streaming Response

---

## Endpoints

POST /ingest
POST /ask
POST /ask-stream

---

## Example `/ask` Response

```json
{
  "success": true,
  "data": {
    "answer": "RAG combines retrieval systems with language models...",
    "sources": [
      "Retrieved chunk 1...",
      "Retrieved chunk 2..."
    ],
    "confidence": 0.81,
    "refused": false
  },
  "meta": {
    "performance": {
      "embed_ms": 142,
      "search_ms": 6,
      "llm_ms": 1203,
      "total_ms": 1351
    }
  }
}
```

---

## Streaming Example

```bash
curl -N -X POST http://127.0.0.1:8000/ask-stream \
-H "Content-Type: application/json" \
-d '{"question":"What is RAG?","top_k":3}'
```

This endpoint streams tokens progressively as they are generated by the local LLM.

---

## Requirements

* Python 3.12+
* Ollama installed locally

Install Ollama:
https://ollama.com

Pull model:

```bash
ollama pull llama3
```

---

## Run Locally

Create virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Run backend:

```bash
uvicorn main:app --reload
```

Backend runs on:

http://127.0.0.1:8000

---

## Why This Project

This backend demonstrates:

* Local LLM inference
* Retrieval-Augmented Generation
* Hybrid search optimization
* Grounding-based hallucination mitigation
* Production API structure
* Observability via performance metrics

It is designed as a portfolio-grade AI backend powering a SwiftUI AI-native mobile application.

---

## Future Improvements

* Persistent vector database (Chroma / FAISS production mode)
* LLM-as-a-judge grounding evaluation
* Authentication layer
* Deployment to cloud environment
* Multi-document indexing with metadata filters

---

Built as part of an AI-Native Engineering portfolio demonstrating end-to-end AI system design.
